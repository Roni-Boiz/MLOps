{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqzB18uanJD7"
   },
   "source": [
    "# Lesson 1.1: ML Pipelines with ZenML\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zenml-io/zenbytes/blob/main/1-1_Pipelines.ipynb)\n",
    "\n",
    "***Key Concepts:*** *ML Pipelines, Steps*\n",
    "\n",
    "In this notebook, we will learn how to easily convert existing ML code into ML pipelines using ZenML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwbSkXUUnJEC"
   },
   "source": [
    "Since we will build models with [sklearn](https://scikit-learn.org/stable/), you will need to have the ZenML sklearn integration installed. You can install ZenML and the sklearn integration with the following command, which will also restart the kernel of your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PuecSrZmnJED",
    "outputId": "702705f9-c9a1-4969-a5d4-97eeb7d271ba"
   },
   "outputs": [],
   "source": [
    "!pip install \"zenml[server]\"\n",
    "!zenml integration install sklearn -y\n",
    "%pip install pyparsing==2.4.2  # required for Colab\n",
    "\n",
    "import IPython\n",
    "\n",
    "# automatically restart kernel\n",
    "# IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjpI2kGGnJEF"
   },
   "source": [
    "**Colab Note:** On Colab, you need an [ngrok account](https://dashboard.ngrok.com/signup) to view some of the visualizations later. Please set up an account, then set your user token below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OnK05CTnJEG"
   },
   "outputs": [],
   "source": [
    "NGROK_TOKEN = \"\"  # TODO: set your ngrok token if you are working on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vp5b9ansnJEG",
    "outputId": "18446be8-5645-489e-9f83-33bf3e17b3f2"
   },
   "outputs": [],
   "source": [
    "from zenml.environment import Environment\n",
    "\n",
    "if Environment.in_google_colab():  # Colab only setup\n",
    "\n",
    "    # install and authenticate ngrok\n",
    "    !pip install pyngrok\n",
    "    !ngrok authtoken {NGROK_TOKEN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GXRpeoinJEH"
   },
   "source": [
    "As an ML practitioner, you are probably familiar with building ML models using Scikit-learn, PyTorch, TensorFlow, or similar. An **[ML Pipeline](https://docs.zenml.io/user-guide/starter-guide)** is simply an extension, including other steps you would typically do before or after building a model, like data acquisition, preprocessing, model deployment, or monitoring. The ML pipeline essentially defines a step-by-step procedure of your work as an ML practitioner. Defining ML pipelines explicitly in code is great because:\n",
    "- We can easily rerun all of our work, not just the model, eliminating bugs and making our models easier to reproduce.\n",
    "- Data and models can be versioned and tracked, so we can see at a glance which dataset a model was trained on and how it compares to other models.\n",
    "- If the entire pipeline is coded up, we can automate many operational tasks, like retraining and redeploying models when the underlying problem or data changes or rolling out new and improved models with CI/CD workflows.\n",
    "\n",
    "Having a clearly defined ML pipeline is essential for ML teams that aim to serve models on a large scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hKk5-d6nJEI"
   },
   "source": [
    "## ZenML Setup\n",
    "Throughout this series, we will define our ML pipelines using [ZenML](https://github.com/zenml-io/zenml/). ZenML is an excellent tool for this task, as it is straightforward and intuitive to use and has [integrations](https://zenml.io/integrations) with most of the advanced MLOps tools we will want to use later. Make sure you have ZenML installed (via `pip install zenml`). Next, let's run some commands to make sure you start with a fresh ML stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1p0NW-cnJEJ",
    "outputId": "1f0321b0-2412-4c0d-cd89-5f82e519f9d7"
   },
   "outputs": [],
   "source": [
    "!rm -rf .zen\n",
    "!zenml init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzqD-K3snJEJ"
   },
   "source": [
    "## Example Experimentation ML Code\n",
    "Let us get started with some simple exemplary ML code. In the following, we train a Scikit-learn SVC classifier to classify images of handwritten digits. We load the data, train a model on the training set, then test it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IAW6vnY4nJEK",
    "outputId": "940a1e29-2e33-45c2-f1bf-e65c2f1e286f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train_test() -> None:\n",
    "    \"\"\"Train and test a Scikit-learn SVC classifier on digits\"\"\"\n",
    "    digits = load_digits()\n",
    "    data = digits.images.reshape((len(digits.images), -1))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data, digits.target, test_size=0.2, shuffle=False\n",
    "    )\n",
    "    model = SVC(gamma=0.001)\n",
    "    model.fit(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "\n",
    "train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkJGEleanJEK"
   },
   "source": [
    "## Turning experiments into ML pipelines with ZenML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktgpiOApnJEK"
   },
   "source": [
    "In practice, your ML workflows will, of course, be much more complicated than that. You might have complex preprocessing that you do not want to redo every time you train a model, you will need to compare the performance of different models, deploy them in a production setting, and much more. Here ML pipelines come into play, allowing us to define our workflows in modular steps that we can then mix and match.\n",
    "\n",
    "![Digits Pipeline](https://github.com/zenml-io/zenbytes/blob/main/_assets/1-1/digits_pipeline.png?raw=1)\n",
    "\n",
    "We can identify three distinct steps in our example: data loading, model training, and model evaluation. Let us now define each of them as a ZenML **[Pipeline Step](https://docs.zenml.io/user-guide/starter-guide)** simply by moving each step to its own function and decorating them with ZenML's `@step` [Python decorator](https://realpython.com/primer-on-python-decorators/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q89dfw9GnJEL"
   },
   "outputs": [],
   "source": [
    "from zenml.steps import step\n",
    "from typing_extensions import Annotated\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import ClassifierMixin\n",
    "from zenml.materializers.numpy_materializer import NumpyMaterializer\n",
    "\n",
    "@step(\n",
    "    output_materializers={\n",
    "        \"X_train\": NumpyMaterializer,\n",
    "        \"X_test\": NumpyMaterializer,\n",
    "        \"y_train\": NumpyMaterializer,\n",
    "        \"y_test\": NumpyMaterializer,\n",
    "    }\n",
    ")\n",
    "def importer() -> Tuple[\n",
    "    Annotated[np.ndarray, \"X_train\"],\n",
    "    Annotated[np.ndarray, \"X_test\"],\n",
    "    Annotated[np.ndarray, \"y_train\"],\n",
    "    Annotated[np.ndarray, \"y_test\"],\n",
    "]:\n",
    "    \"\"\"Load the digits dataset as numpy arrays.\"\"\"\n",
    "    digits = load_digits()\n",
    "    data = digits.images.reshape((len(digits.images), -1))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data, digits.target, test_size=0.2, shuffle=False\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "@step\n",
    "def svc_trainer(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    ") -> ClassifierMixin:\n",
    "    \"\"\"Train an sklearn SVC classifier.\"\"\"\n",
    "    model = SVC(gamma=0.001)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "@step\n",
    "def evaluator(\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    model: ClassifierMixin,\n",
    ") -> float:\n",
    "    \"\"\"Calculate the test set accuracy of an sklearn model.\"\"\"\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    print(f\"Test accuracy: {test_acc}\")\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1T5vmkrSnJEM"
   },
   "source": [
    "Similarly, we can use ZenML's `@pipeline` decorator to connect all of our steps into an ML pipeline.\n",
    "\n",
    "Note that the pipeline definition does not depend on the concrete step functions we defined above; it merely establishes a recipe for how data moves through the steps. This means we can replace steps as we wish, e.g., to run the same pipeline with different models to compare their performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7YlQyudnJEM"
   },
   "outputs": [],
   "source": [
    "from zenml import pipeline\n",
    "\n",
    "@pipeline\n",
    "def digits_pipeline():\n",
    "    \"\"\"Links all the steps together in a pipeline\"\"\"\n",
    "    X_train, X_test, y_train, y_test = importer()\n",
    "    model = svc_trainer(X_train=X_train, y_train=y_train)\n",
    "    evaluator(X_test=X_test, y_test=y_test, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nd1VHZuJnJEM"
   },
   "source": [
    "## Running ZenML Pipelines\n",
    "Finally, we initialize our pipeline with concrete step functions and call the `run()` method to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vA4QQtrmnJEN",
    "outputId": "c8b90407-1a85-427b-e43c-5b3b09e9411f"
   },
   "outputs": [],
   "source": [
    "digits_svc_pipeline = digits_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pth57rLfnJEN"
   },
   "source": [
    "And that's it, we just built and ran our first ML pipeline! Great job!\n",
    "\n",
    "You can now visualize the pipeline run in ZenML's dashboard. To do so, run\n",
    "`zenml up` to spin up a ZenML dashboard locally, log in with username `default`\n",
    "and empty password, and navigate to the \"Runs\" tab in the \"Pipelines\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-XVvoD4nJEN"
   },
   "outputs": [],
   "source": [
    "from zenml.environment import Environment\n",
    "\n",
    "def start_zenml_dashboard(port=8237):\n",
    "    if Environment.in_google_colab():\n",
    "        from pyngrok import ngrok\n",
    "\n",
    "        public_url = ngrok.connect(port)\n",
    "        print(f\"\\x1b[31mIn Colab, use this URL instead: {public_url}!\\x1b[0m\")\n",
    "        !zenml up --blocking --port {port}\n",
    "\n",
    "    else:\n",
    "        !zenml up --port {port}\n",
    "\n",
    "start_zenml_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQZY1LvonJEN"
   },
   "source": [
    "![Viewing Pipeline Runs in the ZenML Dashboard](https://github.com/zenml-io/zenbytes/blob/main/_assets/1-1/view_run_in_dashboard.gif?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLkErnesnJEO"
   },
   "source": [
    "In the [next lesson](1-2_Artifact_Lineage.ipynb), you will see one of the best features of ML pipelines in action: automated artifact versioning and caching. See you there!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('zenbytes-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec45946565c50b1d690aa5a9e3c974f5b62b9cc8d8934e441e52186140f79402"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
